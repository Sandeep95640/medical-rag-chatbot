# ğŸ©º Medical RAG Chatbot

An AI-powered Medical Question Answering system built using **Retrieval-Augmented Generation (RAG)** with **Groq LLaMA 3.1**, **LangChain**, **FAISS**, and **Streamlit**.

ğŸ”— **Live App:**  
https://medical-rag-chatbot-ai.streamlit.app/

---

## ğŸš€ Overview

This application allows users to ask medical-related questions based on a trusted medical encyclopedia (The Gale Encyclopedia of Medicine).

Instead of generating generic AI answers, the system:

- Retrieves relevant sections from the medical PDF
- Sends contextual information to the LLM
- Generates answers strictly grounded in retrieved content
- Displays source document page numbers

This ensures:

- Higher factual accuracy  
- Reduced hallucinations  
- Transparent answer sourcing  

---

## ğŸ§  Architecture

User Question  
â†’ FAISS Vector Search  
â†’ Retrieve Top 3 Relevant Chunks  
â†’ Custom Prompt Template  
â†’ Groq LLaMA 3.1 Model  
â†’ Context-Grounded Answer + Sources  

---

## ğŸ—ï¸ Tech Stack

- **LLM:** Groq (LLaMA 3.1 8B Instant)
- **Framework:** LangChain (LCEL)
- **Embeddings:** Sentence Transformers (all-MiniLM-L6-v2)
- **Vector Database:** FAISS
- **UI:** Streamlit
- **Deployment:** Streamlit Cloud
- **Environment Management:** python-dotenv

---

## ğŸ“‚ Project Structure

```
medical-rag-chatbot/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ create_memory_for_llm.py
â”œâ”€â”€ connect_memory_with_llm.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf
â”‚
â””â”€â”€ vectorstore/
    â””â”€â”€ db_faiss/
        â”œâ”€â”€ index.faiss
        â””â”€â”€ index.pkl
```

---

## âš™ï¸ How It Works

### 1ï¸âƒ£ Document Processing

- Load PDF using `PyPDFLoader`
- Split into 500-token chunks
- Generate embeddings using `all-MiniLM-L6-v2`
- Store embeddings in FAISS vector database

### 2ï¸âƒ£ Retrieval-Augmented Generation (RAG)

- Retrieve top 3 relevant chunks using semantic search
- Inject retrieved context into custom prompt
- Generate answer using Groq LLaMA 3.1
- Return answer strictly based on retrieved content

### 3ï¸âƒ£ Output

- AI-generated response
- Source document references
- Page numbers for transparency

---

## ğŸ” Environment Variables

Create a `.env` file in the root directory:

```
GROQ_API_KEY=your_groq_api_key_here
```

### For Streamlit Cloud Deployment

Go to:

App â†’ Settings â†’ Secrets  

Add:

```
GROQ_API_KEY = "your_groq_api_key_here"
```

---

## ğŸ“¦ Installation (Local Setup)

```bash
git clone https://github.com/Sandeep95640/medical-rag-chatbot.git
cd medical-rag-chatbot

python -m venv venv
venv\Scripts\activate

pip install -r requirements.txt

streamlit run app.py
```

---

## ğŸ§ª Example Queries

- What are the treatment options for cancer?
- What are the treatments and management methods for diabetes?
- What causes hypertension?
- What are symptoms of asthma?

---

## ğŸ“Š Key Features

âœ… Context-grounded responses  
âœ… Page number citations  
âœ… Groq ultra-fast inference  
âœ… FAISS semantic search  
âœ… Clean modular architecture  
âœ… Production-ready Streamlit deployment  

---

## ğŸ¯ Why This Project Matters

This project demonstrates:

- Real-world RAG implementation
- LLM + Vector Database integration
- Context-aware prompting
- Secure deployment with secrets
- Practical GenAI engineering skills

It showcases production-level AI system design beyond basic chatbot applications.

---

## ğŸ”® Future Improvements

- Add conversational memory
- Multi-document upload support
- Highlight exact citation text
- Add PDF preview panel
- Upgrade to larger medical dataset
- Docker-based deployment


---

## â­ If You Found This Helpful

Give this repository a â­ on GitHub!
